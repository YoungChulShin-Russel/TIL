## 구버네티스 구조
컨트롤 플레인 (마스터 영역) - 클러스터 기능을 제어하고 전체 클러스터가 동작하게 만드는 역할을 한다
- API 서버: kube-api-server
   - 쿠버네티스 API를 외부에 공개하는 대규모 REST 서버
   - 클러스터 상태를 etcd에 저장한다
- Etcd 메타데이터 저장소
   - 클러스터 정보가 저장되는 키/값 저장소
   - 중복성을 위해서 etcd 인스턴스 3개 또는 5개가 있는 것이 일반적이다
   - etcd 데이터가 손실되면 클러스터가 손실된다
- 스케쥴러: kube-scheduler
   - 워커 노드에 파드의 스케쥴을 조정한다
- 컨트롤러 매니저: kube-control-manager
   - 여러 컨트롤러를 포함하는 단일 프로세스
      - node controller: 노드가 작동 중단될 때 이를 인지하고 응답하는 일을 담당
      - replica controller: replicaset 에 올바른 수의 파드가 있는지 확인한다
      - endpoint controller: 각 서비스에 서비스 파드를 나열하는 엔드포인트 오브젝트를 할당한다
      - service account, token controller
- 클라우드 컨트롤러 매니저

데이터 플레인 (노드 영역) - 실제 배포되는 컨테이너 애플리케이션을 실행한다
- kubelet: 쿠버네티스 에이전트
   - API 서버에서 파드 시크릿 다운로드
   - 볼륨 마운트
   - 컨테이너 런타임 인터페이스로 파드 컨테이너 실행
   - 노드와 각 파드의 상태보고
   - 컨테이너 라이프사이클 검사
- kube proxy
   - 노드의 네트워킹을 담당
   - 애플리케이션 구성요소 간에 네트워크 트래픽을 로드밸런싱하는 서비스 프록시


컨테이너 런타임
- kubectl: 쿠버네티스 클러스터에 대한 CLI

## 쿠버네티스 내부 이해
구성 요소가 통신하는 방법
- 구성요소들은 오직 API 서버하고만 통신한다. 서로 직접 통신하지 않는다
- etcd에는 클러스터의 정보가 저장되는데, API 서버를 통해서 통신한다.
   - 이를 통해서 낙관적 잠금으로 데이터 저장시 충돌을 피할 수 있다

병렬 실행
- 워커노드의 구성요소(kubelet, kube proxy, container runtime)는 모두 동일한 노드에서 실행돼야 한다
- 컨트롤 플레인 구성요소는 여러 서버에 걸쳐 실행될 수 있다
   - etcd, api 서버는 여러 인스턴스를 동시에 활성화해서 병렬로 수행할 수 있다
   - scheduler, controller manager는 하나의 인스턴스만 활성화된다

### etcd
기능
- 모든 오브젝트(pod, replication controller, service, secret 등)는 API 서버가 다시 사직되거나 실패하더라도 메니페스트 정보를 유지하기 위해서 etcd를 사용한다
- 분산되어 있고, 둘 이상의 etcd 인스턴스를 실행해 고가용성을 제공한다
- API 서버만 etcd와 통신한다
- 리소스의 완전한 JSON 표현을 etcd에 저장한다

### API server
기능
- 다른 모든 구성요소와 kubectl 같은 클라이언트에서 사용하는 중심 구성 요소
- 클러스터의 상태를 조회/변경하기 위해서 RESTful API로 CRUD 인터페이스를 제공한다. 상태를 etcd에 저장된다.

동작 순서
1. kubectl 을 이용해서 요청
2. HTTP Post로 API server에 요청
3. 인증 프로세스 
4. 인가 프로세스
5. 어드미션 컨트롤 프로세스
6. 리소스 유효성 검사
7. etcd에 저장

API 서버 갱신
- 클라이언트는 API 서버에 HTTP 연결을 맺고 변경 사항을 감지한다. 이 연결을 통해서 클라이언트는 감시 대상 오브젝트의 변경을 알 수 있는 스트림을 받는다. 
- 예를 들어서, kubectl 을 이용해서 오브젝트를 변경했을 때는 아래와 같이 동작을 한다
   1. kubectl은 API 서버에 변경 요청을 HTTP로 전달한다
   2. API 서버는 변경 내용을 etcd에 저장한다
   3. etcd가 API 서버로 변경 내용을 통보한댜
   4. 변경 사항을 감지하고 있던 클라이언트에게 변경 사항이 전달된다
- `--watch` 옵션을 이용해서 파드의 생성,수정,삭제 통보를 받을 수 있다
   ```
   kubectl get pods --watch
   ```

### 스케쥴러
기능
- 파드를 실행할 때 노드를 선택하지 않는데, 이는 스케쥴러에게 맡겨진 일이다
- API 서버의 감시 매커니즘을 통해 새로 생성될 파드를 기다리고 있다가, 할당된 노드가 없는 파드가 있으면 여기에 노드를 할당한다
- 스케쥴러가 노드에 파드를 실행하도록 지시하는 것은 아니고, API 서버에 파드의 노드 정보를 갱신한다.<br>API 서버에서 파드 정보가 갱신되면, 워커 노드의 Kubelet이 감시 매커니즘을 통해서 파드가 노드에 스케쥴링된 것을 확인하고, 컨테이너를 생성 및 실행한다.

스케쥴링 알고리즘
1. 스케쥴러가 전체 노드를 대상으로 파드를 스케쥴링 할 수 있는 노드 목록을 필터링
2. 수행 가능한 노드 중에서 우선 순위를 정해서 최상위 점수를 가진 노드를 선택
   - 여러 노드가 같은 최상위 점수를 가진다면 라운드-로빈 방식을 사용

### 컨트롤러 매니저
종류
- Relication Manager
- Replicaset, DaemonSet, Job Controller
- Deployment Controller
- Statefulset Controller
- Node Controller
- Service Controller: LoadBalancer 유형의 서비스가 생성되거나 삭제될 때 인프라스트럭쳐에 LoadBalancer를 요청하고 해제하는 역할
- Endpoint Controller: LabelSelector와 일치하는 파드의 IP/Port 정보를 엔드포인트로 계속 관리
- Namesapce Controller
- PV Controller
- 기타

기능
- API 서버에서 리소스가 변경되는 것을 감시하고 각 변경 작업을 수행한다. 다른 리소스 생성, 감시 중인 리소스 자체를 갱신하는 것이 포함된다.
- 각 컨트롤러는 서로 대화하지는 않고, API 서버와 통신한다

동작 방식의 개념
- 컨트롤러는 API 서버를 통해서 API 오브젝트를 제어한다. 직접적으로 Kubelet과 직접 통신하지는 않는다.
- kubelet은 변경된 리소스를 파악해서 각자의 작업을 수행한다

### Kubelet
기능
- 워커노드에서 실행되는 모든것을 담당하는 구성요소
- 파드 스케쥴링
   1. kubelet이 실행중인 노드를 노드 리소스로 만들어서 API 서버에 등록한다
   2. API 서버를 모니터링
   3. 스케쥴러에 의해서 노드에 퍄드가 스케쥴링되면 파드의 컨테이너를 시작한다
      - 설정된 컨테이너 런타임에 지정된 컨테이너 이미지로 컨테이너를 실행하도록 지시한다
   4. 실행중인 컨테이너를 계속 모니터링하면서 상태,이벤트,리소스 사용량 등을 API 서버에 보낸다
- 파드 체크
   - Liveness Probe를 실행하는 구성요소이고, 실패할 경우 컨테이너를 다시 시작한다
- 파드 삭제
   - API 서버에서 파드가 삭제되면 컨테이너를 정지하고 파드가 종료된 것을 API 서버에 통보한다

### Kube-proxy
기능
- 서비스의 IP/Port로 들어온 접속을 서비스를 지원하는 파드중 하나와 연결시켜준다

iptables 프록시 모드
- 클라이언트 <<-->> iptables <<-->> Pod
   - kube-proxy를 거치지 않는다
   - kernelspace에서 실행된다 (userspace 보다 빠르다)
- kubeproxy는 iptables에 정보를 업데이트한다
- 파드를 무작위로 선택하기 때문에 소수의 파드를 운영할 경우에 고르게 분배되지 않을 수 있다

### DNS Server
기능
- 클러스터의 내부 DNS 서버
- 서비스를 이름으로 쉽게 찾을 수 있다. Headless 서비스 파드라면 파드의 IP주소를 조회할 수 있다.
- `kube-dns` 서비스로 노출된다.
   - dns 서비스는 `kube-system` 네임스페이스에 서비스로 존재한다
   - 서비스이기 때문에 워커 노드에 서비스가 가리키는 파드가 있다

동작 방법
- `kube-dns` 파드는 API 서버 감시 매커니즘을 이용해서 서브스와 엔드포인트 변화를 관찰하고 모든 변화를 DNS 레코드에 갱신한다

kube-dns 정보
- /etc/resolv.conf 파일에 nameserver로 저장되어 있다

### Ingress
기능
- 리버스 프록시 서버(예: nginx)를 실행
- 리소스의 정의는 서비스를 가리키지만, 컨트롤러는 트래픽을 서비스의 IP로 보내지 않고 파드로 직접 전달한다

### POD가 생성되는 과정의 동작흐름
`Kubernetes in Action` 책에서 507페이지를 참고.

Pod 생성에 대한 동작 흐름
1. [컨트롤플레인] kubelet을 통해서 deployment 리소스를 생성
2. [컨트롤플레인] Deployment-Controller는 API서버에서 리소스 생성에 대한 통보를 받고, ReplicaSet을 생성
   - Kuberenetes API로 Replicaset을 생성
3. [컨트롤플레인] Replicaset-Controller는 ReplicaSet 리소스 생성에 대한 통보를 받고, Replicaset의 PodTemplate을 기반으로 파드 리소스를 생성
   - Pod는 etcd에 저장
4. [컨트롤플레인] Scheduler는 nodeName이 없는 Pod를 발견하고, 적합한 노드를 찾아서 Pod에 노드를 할당
5. [워커노드] Kubelet은 노드에 스케줄링된 새 Pod를 발견하면, 컨테이너런타임에 Pod의 컨테이너를 시작하도록 지시
5. [워커노드] 컨테이너런타임이 컨테이너를 시작

이벤트
- 앞에서 설명한 각각의 동작이 일어날 때마다 컨트롤플레인 구성요소(Controller, Scheduler)와 Kubelet은 API 서버로 이벤트를 발송한다
- 이벤트 확인
   ```
   1. kubectl describe 로 확인
   2. kubectl get events --watch 로 확인
   ```

### 인프라스트럭처 컨테이너
기능
- Pause 컨테이너
- 파드의 리눅스 네임스페이스 정보를 보유
- 파드의 라아프사이클과 동일하다

### 파드간 네트워크 연결
네트워크
- 파드가 동일한 워커 노드에서 실행 중인지 여부와 관계없이 파드끼리 서로 통신할 수 있어야한다
   - 파드 A가 네트워크 패킷을 보내기 위해 파드 B에 연결할 때, 파드 B가 보는 출발지 IP는 파드 A의 IP주소와 동일해야 한다

동일한 노드에서 파드간의 통신
![SingleNode](/kubernetes/images/basic_network_singlenode.png)
- 컨테이너를 위한 가상 이더넷 인터페이스 쌍이 생성된다.
   - 하나는 호스트의 네임스페이스
   - 나머지 하나는 컨테이너의 네임스페이스
- 호스트의 인터페이스는 노드의 브리지에 연결된다
- 노드에 있는 모든 컨테이너는 브리지에 연결되어 있기 때문에 통신할 수 있다

다른 노드에서 파드간의 통신
![MultiNode](/kubernetes/images/basic_network_multinode.png)
- 파드의 IP 주소는 클러스터 내에서 유일해야 하기 때문에 노드 사이의 브리지는 겹치지 않는 주소 범위를 사용해서 이를 막아준다
- eth0은 노드의 물리 어댑터

### kube-proxy, iptables
kube-proxy
- 서비스와 엔드포인트의 변경을 감시하고, 결과를 노드의 iptable에 업데이트 한다

endpoint object
- 서비스를 지원하는 모드 pod의 ip/port 쌍을 가지고 있다

iptables
- kube-proxy에 의해서 업데이트
- service에 대한 파드의 ip/port 정보가 있는 것 같다
- 노드에 존재

파드 A에서 파드 B로 패킷 전달<br>
![iptables](/kubernetes/images/basic_iptables.png)
1. 파드A는 출발지를 자기 AP, 도착지를 서비스B의 IP/Port로 전달
2. iptables에 서비스B에 대한 규칙이 등록되어 있기 때문에, 임의의 파드의 IP와Port로 패킷을 변경
3. 패킷이 변경된 파드의 IP/Port로 전달

### 고가용성 클러스터 실행
애플리케이션 가용성
- 다양한 컨트롤러를 통해서 노드 장애가 발생해도 애플리케이션이 특졍 규모로 원할하게 동작할 수 있게 해준다.

컨트롤-플레인 가용성
- 여러대의 마스터 노드를 실행해서 가용성을 높일 수 있다
- etcd
   - 분산 시스템으로 설계됐으므로 가용성을 높이는 것이 큰 문제가 되지 않는다
   - etcd는 모든 인스턴스에 걸쳐 데이터를 복제하기 때문에 세 대의 머신으로 구성된 클러스터는 한 노드가 실패해도 읽기와/쓰기 작업을 모두 수행할 수 있다
- API 서버
   - API 서버는 상태를 가지고 있지 않기 때문에 필요한 만큼 띄워서 실행할 수 있다
   - 보통 etcd 인스턴스에 함께 띄워서 모든 API 서버가 local에 있는 인스턴스와 통신하게 한다
   - API 서버 앞에는 로드밸런서가 위치하고, 클라이언트(kubectl, kubelet, contoller manager, scheduler)는 항상 정상적인 인스턴스에게만 연결된다
- 컨트롤러/스케줄러
   - 동시에 실행되지는 않고, 하나가 실행될 동안 나머지는 대기한다
   - 상태를 감시하고 변경될 때 반응해야하는데 여러 인스턴스가 동시에 실행되면 이 작업을 하기가 쉽지 않다. 